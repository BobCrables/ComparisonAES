{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Never do this, but warnings are annoying so I'm blocking them\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats\n",
    "\n",
    "from skll.metrics import kappa\n",
    "from bhkappa import mean_quadratic_weighted_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-dde6b47f03a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfp1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove.6B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mglove_emb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mglove_emb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "texts=[]\n",
    "labels=[]\n",
    "sentences=[]\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  0.0  ; range max -  4.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '5'\n",
    "\n",
    "fp=open(\"data/training_set.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "fp.close()\n",
    "\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "fp=open(\"data/training_set.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1805\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5002 unique tokens.\n",
      "Shape of data tensor: (1805, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer() #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=data[:-validation_size]\n",
    "y_train=labels[:-validation_size]\n",
    "x_val=data[-validation_size:]\n",
    "y_val=labels[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "vocab_size=len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=5, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "\te = Input(name='essay',shape=(maxlen,))\n",
    "\ttrad_feats=Input(shape=(7,))\n",
    "\tembed = embedding_layer(e)\n",
    "\tlstm_layer=LSTM(lstm_dim,return_sequences=True) \n",
    "\thidden_states=lstm_layer(embed)\n",
    "\thtm=Temporal_Mean_Pooling()(hidden_states)    \n",
    "\tside_embed = side_embedding_layer(e)\n",
    "\tside_hidden_states=lstm_layer(side_embed)   \n",
    "\ttensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
    "\tpairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "\thidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "\tsigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "\tcoherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "\tco_tm=Concatenate()(coherence[:]+[htm])\n",
    "\tdense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "\tdense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tdense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tout = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\tmodel = Model(inputs=[e], outputs=[out]) # inputs=[e,trad_feats]\n",
    "\tadam = Adam(lr=lr, decay=lr_decay)\n",
    "\tmodel.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'neural__tensor_layer_2/Variable:0' shape=(4, 50, 50) dtype=float32> W\n",
      "tracking <tf.Variable 'neural__tensor_layer_2/Variable_1:0' shape=(100, 4) dtype=float32> V\n",
      "tracking <tf.Variable 'neural__tensor_layer_2/Variable_2:0' shape=(50,) dtype=float32> b\n"
     ]
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_MSE\", patience=5)\n",
    "sf_1 = SKIPFLOW(lstm_dim=50, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1444 samples, validate on 361 samples\n",
      "Epoch 1/100\n",
      "1444/1444 [==============================] - 50s 35ms/step - loss: 0.0576 - MSE: 0.0576 - val_loss: 0.0542 - val_MSE: 0.0542\n",
      "Epoch 2/100\n",
      "1444/1444 [==============================] - 47s 33ms/step - loss: 0.0419 - MSE: 0.0419 - val_loss: 0.0335 - val_MSE: 0.0335\n",
      "Epoch 3/100\n",
      "1444/1444 [==============================] - 47s 33ms/step - loss: 0.0271 - MSE: 0.0271 - val_loss: 0.0254 - val_MSE: 0.0254\n",
      "Epoch 4/100\n",
      "1444/1444 [==============================] - 68s 47ms/step - loss: 0.0222 - MSE: 0.0222 - val_loss: 0.0217 - val_MSE: 0.0217\n",
      "Epoch 5/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0210 - MSE: 0.0210 - val_loss: 0.0208 - val_MSE: 0.0208\n",
      "Epoch 6/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0198 - MSE: 0.0198 - val_loss: 0.0203 - val_MSE: 0.0203\n",
      "Epoch 7/100\n",
      "1444/1444 [==============================] - 47s 32ms/step - loss: 0.0190 - MSE: 0.0190 - val_loss: 0.0198 - val_MSE: 0.0198\n",
      "Epoch 8/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0179 - MSE: 0.0179 - val_loss: 0.0182 - val_MSE: 0.0182\n",
      "Epoch 9/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0174 - MSE: 0.0174 - val_loss: 0.0176 - val_MSE: 0.0176\n",
      "Epoch 10/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0167 - MSE: 0.0167 - val_loss: 0.0230 - val_MSE: 0.0230\n",
      "Epoch 11/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0170 - MSE: 0.0170 - val_loss: 0.0171 - val_MSE: 0.0171\n",
      "Epoch 12/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0159 - MSE: 0.0159 - val_loss: 0.0174 - val_MSE: 0.0174\n",
      "Epoch 13/100\n",
      "1444/1444 [==============================] - 46s 32ms/step - loss: 0.0164 - MSE: 0.0164 - val_loss: 0.0178 - val_MSE: 0.0178\n",
      "Epoch 14/100\n",
      "1444/1444 [==============================] - 45s 31ms/step - loss: 0.0152 - MSE: 0.0152 - val_loss: 0.0182 - val_MSE: 0.0182\n",
      "Epoch 15/100\n",
      "1444/1444 [==============================] - 45s 31ms/step - loss: 0.0148 - MSE: 0.0148 - val_loss: 0.0183 - val_MSE: 0.0183\n",
      "Epoch 16/100\n",
      "1444/1444 [==============================] - 45s 31ms/step - loss: 0.0151 - MSE: 0.0151 - val_loss: 0.0201 - val_MSE: 0.0201\n"
     ]
    }
   ],
   "source": [
    "epochs = 100    # 1000 has also been suggested\n",
    "hist = sf_1.fit([x_train], y_train, batch_size=32, epochs=epochs,   # Used to be 1024\n",
    "                validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=sf_1.predict([x_val])\n",
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(361).tolist()] #Change this to match output shape\n",
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save('models/5_model.h5')\n",
    "sf_1.save_weights('models/5_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2809587 ],\n",
       "       [2.9796457 ],\n",
       "       [1.967578  ],\n",
       "       [3.6454659 ],\n",
       "       [1.3921044 ],\n",
       "       [3.231636  ],\n",
       "       [2.7819629 ],\n",
       "       [2.7494287 ],\n",
       "       [1.9229802 ],\n",
       "       [2.3569622 ],\n",
       "       [3.7367249 ],\n",
       "       [3.051389  ],\n",
       "       [3.6930866 ],\n",
       "       [2.5271583 ],\n",
       "       [1.542272  ],\n",
       "       [2.8635821 ],\n",
       "       [1.4765416 ],\n",
       "       [2.4264169 ],\n",
       "       [3.4015884 ],\n",
       "       [2.6793268 ],\n",
       "       [2.9679804 ],\n",
       "       [1.8038375 ],\n",
       "       [3.6541696 ],\n",
       "       [0.96453047],\n",
       "       [1.4069722 ],\n",
       "       [1.6748512 ],\n",
       "       [2.834257  ],\n",
       "       [3.03832   ],\n",
       "       [1.5418419 ],\n",
       "       [2.2322204 ],\n",
       "       [1.9243249 ],\n",
       "       [2.4652753 ],\n",
       "       [3.2020397 ],\n",
       "       [3.2474005 ],\n",
       "       [1.871345  ],\n",
       "       [3.2284288 ],\n",
       "       [2.1611218 ],\n",
       "       [3.227532  ],\n",
       "       [1.9946692 ],\n",
       "       [3.02138   ],\n",
       "       [2.8512318 ],\n",
       "       [1.9411013 ],\n",
       "       [1.0613296 ],\n",
       "       [2.8964298 ],\n",
       "       [3.0888174 ],\n",
       "       [1.1007787 ],\n",
       "       [3.2301497 ],\n",
       "       [0.7759099 ],\n",
       "       [1.727255  ],\n",
       "       [1.7162793 ],\n",
       "       [2.048013  ],\n",
       "       [1.2927651 ],\n",
       "       [2.2746315 ],\n",
       "       [3.6161132 ],\n",
       "       [2.5135982 ],\n",
       "       [2.393404  ],\n",
       "       [2.1199777 ],\n",
       "       [0.9534919 ],\n",
       "       [1.8686014 ],\n",
       "       [1.0186093 ],\n",
       "       [0.8136898 ],\n",
       "       [0.63884866],\n",
       "       [1.7054322 ],\n",
       "       [1.5603933 ],\n",
       "       [2.925867  ],\n",
       "       [3.7051284 ],\n",
       "       [2.2633576 ],\n",
       "       [3.48765   ],\n",
       "       [2.0366673 ],\n",
       "       [3.1045973 ],\n",
       "       [2.2993634 ],\n",
       "       [2.2214112 ],\n",
       "       [2.2834568 ],\n",
       "       [2.084985  ],\n",
       "       [1.691348  ],\n",
       "       [1.5407951 ],\n",
       "       [3.688253  ],\n",
       "       [3.359393  ],\n",
       "       [2.6621566 ],\n",
       "       [0.9649503 ],\n",
       "       [2.295251  ],\n",
       "       [1.4537811 ],\n",
       "       [3.534721  ],\n",
       "       [1.6033995 ],\n",
       "       [0.77899575],\n",
       "       [1.3726788 ],\n",
       "       [2.4353144 ],\n",
       "       [1.6292562 ],\n",
       "       [1.5058017 ],\n",
       "       [1.6390569 ],\n",
       "       [1.7327788 ],\n",
       "       [2.8242266 ],\n",
       "       [3.6239715 ],\n",
       "       [0.92288876],\n",
       "       [3.644885  ],\n",
       "       [1.7737646 ],\n",
       "       [2.6193964 ],\n",
       "       [0.86752486],\n",
       "       [2.3427553 ],\n",
       "       [1.8328345 ],\n",
       "       [1.4937534 ],\n",
       "       [3.1677973 ],\n",
       "       [0.9073101 ],\n",
       "       [0.99197125],\n",
       "       [0.99332917],\n",
       "       [1.9377904 ],\n",
       "       [1.0406992 ],\n",
       "       [0.9753386 ],\n",
       "       [1.5513299 ],\n",
       "       [2.3823485 ],\n",
       "       [3.232933  ],\n",
       "       [2.3493357 ],\n",
       "       [3.3649573 ],\n",
       "       [2.3091197 ],\n",
       "       [2.526615  ],\n",
       "       [2.5429997 ],\n",
       "       [2.2703373 ],\n",
       "       [1.0879475 ],\n",
       "       [3.1804166 ],\n",
       "       [1.7247151 ],\n",
       "       [2.7630134 ],\n",
       "       [2.9040463 ],\n",
       "       [0.59467256],\n",
       "       [2.988816  ],\n",
       "       [3.0969481 ],\n",
       "       [1.8166418 ],\n",
       "       [1.545305  ],\n",
       "       [1.8137167 ],\n",
       "       [1.7735385 ],\n",
       "       [1.6230279 ],\n",
       "       [1.0995822 ],\n",
       "       [2.1776006 ],\n",
       "       [3.4590654 ],\n",
       "       [1.9722826 ],\n",
       "       [3.0942152 ],\n",
       "       [1.8914572 ],\n",
       "       [3.671605  ],\n",
       "       [1.6673892 ],\n",
       "       [0.586066  ],\n",
       "       [1.3692017 ],\n",
       "       [3.537682  ],\n",
       "       [0.9955586 ],\n",
       "       [1.5940224 ],\n",
       "       [0.59545124],\n",
       "       [0.9016719 ],\n",
       "       [1.8449455 ],\n",
       "       [2.9091187 ],\n",
       "       [2.146815  ],\n",
       "       [3.5829334 ],\n",
       "       [1.4600506 ],\n",
       "       [0.83682   ],\n",
       "       [2.935863  ],\n",
       "       [2.6567922 ],\n",
       "       [2.317934  ],\n",
       "       [2.2796998 ],\n",
       "       [0.6933185 ],\n",
       "       [2.3588605 ],\n",
       "       [1.7467148 ],\n",
       "       [2.0202541 ],\n",
       "       [1.7427276 ],\n",
       "       [1.790751  ],\n",
       "       [0.6983081 ],\n",
       "       [1.6881893 ],\n",
       "       [2.3111866 ],\n",
       "       [1.6022434 ],\n",
       "       [1.8577422 ],\n",
       "       [3.1580958 ],\n",
       "       [2.6509914 ],\n",
       "       [1.629042  ],\n",
       "       [2.593407  ],\n",
       "       [1.7830343 ],\n",
       "       [2.1436431 ],\n",
       "       [2.1189995 ],\n",
       "       [3.7278585 ],\n",
       "       [3.5028586 ],\n",
       "       [0.8595972 ],\n",
       "       [0.68737245],\n",
       "       [3.5867903 ],\n",
       "       [2.3073547 ],\n",
       "       [1.7612697 ],\n",
       "       [2.197826  ],\n",
       "       [1.7890882 ],\n",
       "       [1.7939819 ],\n",
       "       [1.6656954 ],\n",
       "       [2.9723673 ],\n",
       "       [2.4154792 ],\n",
       "       [3.0843365 ],\n",
       "       [2.371086  ],\n",
       "       [0.9607309 ],\n",
       "       [2.135368  ],\n",
       "       [1.8372548 ],\n",
       "       [2.923697  ],\n",
       "       [3.413196  ],\n",
       "       [0.81504154],\n",
       "       [1.6771736 ],\n",
       "       [3.170291  ],\n",
       "       [3.2899146 ],\n",
       "       [2.105825  ],\n",
       "       [3.5949903 ],\n",
       "       [2.0629904 ],\n",
       "       [2.541627  ],\n",
       "       [1.818655  ],\n",
       "       [3.144845  ],\n",
       "       [2.1047664 ],\n",
       "       [2.4039953 ],\n",
       "       [2.8673663 ],\n",
       "       [3.220274  ],\n",
       "       [2.8356264 ],\n",
       "       [2.8277802 ],\n",
       "       [3.4282763 ],\n",
       "       [2.9221451 ],\n",
       "       [1.8373535 ],\n",
       "       [0.86754143],\n",
       "       [2.853897  ],\n",
       "       [2.621467  ],\n",
       "       [1.5063276 ],\n",
       "       [2.4620616 ],\n",
       "       [1.2972391 ],\n",
       "       [0.59787023],\n",
       "       [3.3211598 ],\n",
       "       [1.5250582 ],\n",
       "       [2.868223  ],\n",
       "       [1.4573889 ],\n",
       "       [2.4993706 ],\n",
       "       [0.78515744],\n",
       "       [1.298786  ],\n",
       "       [2.2120576 ],\n",
       "       [1.6264523 ],\n",
       "       [1.4953556 ],\n",
       "       [2.4538174 ],\n",
       "       [0.74300003],\n",
       "       [2.8819244 ],\n",
       "       [0.55573964],\n",
       "       [1.734468  ],\n",
       "       [3.27848   ],\n",
       "       [3.2624645 ],\n",
       "       [1.5474786 ],\n",
       "       [2.7922854 ],\n",
       "       [2.0406835 ],\n",
       "       [2.5031395 ],\n",
       "       [1.4522228 ],\n",
       "       [3.2420645 ],\n",
       "       [1.4566591 ],\n",
       "       [2.5370042 ],\n",
       "       [2.255646  ],\n",
       "       [3.451274  ],\n",
       "       [2.0521083 ],\n",
       "       [1.6691836 ],\n",
       "       [3.0407393 ],\n",
       "       [3.1138513 ],\n",
       "       [1.1177362 ],\n",
       "       [2.151974  ],\n",
       "       [1.2627023 ],\n",
       "       [2.9551568 ],\n",
       "       [1.8041177 ],\n",
       "       [3.394908  ],\n",
       "       [0.7320483 ],\n",
       "       [2.0677059 ],\n",
       "       [0.91107976],\n",
       "       [3.3208137 ],\n",
       "       [2.2247262 ],\n",
       "       [3.5290623 ],\n",
       "       [1.5000737 ],\n",
       "       [2.2901478 ],\n",
       "       [0.9092016 ],\n",
       "       [2.4109561 ],\n",
       "       [1.5458934 ],\n",
       "       [1.9379013 ],\n",
       "       [0.80288553],\n",
       "       [1.9281712 ],\n",
       "       [1.2462965 ],\n",
       "       [2.1422303 ],\n",
       "       [0.9576701 ],\n",
       "       [2.3500135 ],\n",
       "       [3.0741537 ],\n",
       "       [0.92683196],\n",
       "       [1.8241165 ],\n",
       "       [3.0008361 ],\n",
       "       [1.2477131 ],\n",
       "       [2.801732  ],\n",
       "       [0.7612574 ],\n",
       "       [1.5644996 ],\n",
       "       [2.5533094 ],\n",
       "       [2.0095625 ],\n",
       "       [1.7450942 ],\n",
       "       [2.2566493 ],\n",
       "       [3.0920095 ],\n",
       "       [2.7119913 ],\n",
       "       [1.192594  ],\n",
       "       [3.215258  ],\n",
       "       [3.3534899 ],\n",
       "       [1.6937226 ],\n",
       "       [2.4113264 ],\n",
       "       [3.1018581 ],\n",
       "       [1.7132206 ],\n",
       "       [2.0976644 ],\n",
       "       [2.2753987 ],\n",
       "       [1.4857502 ],\n",
       "       [2.0106854 ],\n",
       "       [1.35677   ],\n",
       "       [2.6970334 ],\n",
       "       [3.663744  ],\n",
       "       [1.4469626 ],\n",
       "       [2.2113926 ],\n",
       "       [3.0515447 ],\n",
       "       [1.5701139 ],\n",
       "       [1.0037544 ],\n",
       "       [3.2883673 ],\n",
       "       [1.2567303 ],\n",
       "       [1.5682857 ],\n",
       "       [1.500231  ],\n",
       "       [2.7480555 ],\n",
       "       [0.47410083],\n",
       "       [3.688857  ],\n",
       "       [1.6600844 ],\n",
       "       [2.8960042 ],\n",
       "       [1.0569782 ],\n",
       "       [3.11861   ],\n",
       "       [1.7649515 ],\n",
       "       [1.1179047 ],\n",
       "       [1.2886405 ],\n",
       "       [2.3797429 ],\n",
       "       [1.1360211 ],\n",
       "       [0.569826  ],\n",
       "       [1.4898155 ],\n",
       "       [2.8963475 ],\n",
       "       [3.6296878 ],\n",
       "       [1.9803579 ],\n",
       "       [1.1058636 ],\n",
       "       [0.71936   ],\n",
       "       [2.9991744 ],\n",
       "       [1.7891101 ],\n",
       "       [2.4888654 ],\n",
       "       [2.0673213 ],\n",
       "       [3.218337  ],\n",
       "       [1.6069771 ],\n",
       "       [1.5394514 ],\n",
       "       [3.610887  ],\n",
       "       [1.1714282 ],\n",
       "       [2.065891  ],\n",
       "       [3.2177222 ],\n",
       "       [1.7125382 ],\n",
       "       [2.6605158 ],\n",
       "       [3.133917  ],\n",
       "       [2.4961748 ],\n",
       "       [2.23693   ],\n",
       "       [2.9952397 ],\n",
       "       [1.0580833 ],\n",
       "       [2.4882886 ],\n",
       "       [2.8024585 ],\n",
       "       [3.3774867 ],\n",
       "       [3.0469215 ],\n",
       "       [3.159964  ],\n",
       "       [0.7189989 ],\n",
       "       [3.70952   ],\n",
       "       [0.91619587],\n",
       "       [0.76582277],\n",
       "       [3.4645293 ],\n",
       "       [3.5007381 ],\n",
       "       [3.6734273 ],\n",
       "       [2.213875  ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'errno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1877\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[0;32m   1879\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-53dfd81e5f97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msf_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[1;32m--> 240\u001b[1;33m                        expand_nested, dpi)\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Attempt to create an image of a blank graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         raise OSError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0;32m   1879\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m                 raise Exception(\n\u001b[0;32m   1882\u001b[0m                     '\"{prog}\" not found in path.'.format(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'errno'"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
